{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fakenws.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHFdmmEpgs7eDiyJVM1t9B",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raheedm/c-programs/blob/main/Fakenews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uC-WF_TRG1U5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import nltk #Import NLTK ---> Natural Language Toolkit\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "INPUTFILE = r'fake_news_train.csv'"
      ],
      "metadata": {
        "id": "O8zD_7UFHBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_ratio = 0.1"
      ],
      "metadata": {
        "id": "uc2a8Ka1HJPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the file for analysis. Change the path of the file if required\n",
        "df = pd.read_csv(INPUTFILE, encoding='utf-8',header=0)"
      ],
      "metadata": {
        "id": "E3naGb7aHOqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wy_ws1q7HUlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'] = np.where(df['label'] == 'fake', 1, 0)"
      ],
      "metadata": {
        "id": "AgWEI1_HHVlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "SHzrQbXTHaW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot the distribution of the label column in the data\n",
        "cat_target=df.label\n",
        "label_size = [cat_target.sum(),len(cat_target)-cat_target.sum()]\n",
        "plt.pie(label_size,explode=[0.1,0.1],colors=['darkgrey','darkorange'],startangle=90,shadow=True,labels=['fake','real'],autopct='%1.1f%%')"
      ],
      "metadata": {
        "id": "kWto59_oHbLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to preprocess the data\n",
        "def preprocess_data(data):\n",
        "  \n",
        "  # 1. Tokenization\n",
        "    tk = RegexpTokenizer('\\s+', gaps = True)\n",
        "    text_data = [] # List for storing the tokenized data\n",
        "    for values in data.news_text:\n",
        "        tokenized_data = tk.tokenize(values) # Tokenize the news\n",
        "        text_data.append(tokenized_data) # append the tokenized data\n",
        "\n",
        "  # 2. Stopword Removal\n",
        "\n",
        "  # Extract the stopwords\n",
        "    sw = stopwords.words('english')\n",
        "    clean_data = [] # List for storing the clean text\n",
        "  # Remove the stopwords using stopwords\n",
        "    for data in text_data:\n",
        "        clean_text = [words.lower() for words in data if words.lower() not in sw]\n",
        "        clean_data.append(clean_text) # Appned the clean_text in the clean_data list\n",
        "  \n",
        "  # 3. Stemming\n",
        "\n",
        "  # Create a stemmer object\n",
        "    ps = PorterStemmer()\n",
        "    stemmed_data = [] # List for storing the stemmed data\n",
        "    for data in clean_data:\n",
        "        stemmed_text = [ps.stem(words) for words in data] # Stem the words\n",
        "        stemmed_data.append(stemmed_text) # Append the stemmed text\n",
        "  \n",
        "\n",
        "  # 4. tfidf vectorizer --> Term Frequency Inverse Document Frequency  \n",
        "  # Flatten the stemmed data\n",
        "\n",
        "    updated_data = []\n",
        "    for data in stemmed_data:\n",
        "        updated_data.append(\" \".join(data))\n",
        "\n",
        "  # TFID Vector object\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(updated_data)\n",
        "\n",
        "    return tfidf_matrix"
      ],
      "metadata": {
        "id": "JLnOeJHBHg78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the text data\n",
        "preprocessed_data = preprocess_data(df.drop('label', axis=1))"
      ],
      "metadata": {
        "id": "EbD7P03XHlWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = pd.DataFrame(preprocessed_data.toarray())\n",
        "datadf = pd.concat([features_df,df['label']],axis=1)"
      ],
      "metadata": {
        "id": "Xw0Zc-U9Ho2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train & test in the ratio of 80:20\n",
        "from sklearn.model_selection import train_test_split \n",
        "X_train, X_test, y_train, y_test = train_test_split(datadf, datadf.label, test_size=test_data_ratio, random_state = 42)"
      ],
      "metadata": {
        "id": "vaV5cW_YH6JL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metrics for Classification models\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Function for deriving a classification report & Confusion matrix for any algorithm\n",
        "def compute_metrics(data, y_true, model_obj, model):\n",
        "\n",
        "        # Make predictions\n",
        "    y_pred = model_obj.predict(data)\n",
        "    print(metrics.classification_report(y_true, y_pred,target_names = ['real', 'fake']))\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred, labels=model_obj.classes_)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['real', 'fake'])\n",
        "    disp.plot()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bmrDTeISH-IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import LogisticRegression model from sklearn library\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "# Initialize the model \n",
        "lr_reg = LogisticRegressionCV(Cs=20, cv=3, random_state=42)\n",
        "\n",
        "# fit the model\n",
        "lr_reg.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NF_FvNTTH_Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compute the Evaluation Metrics for train set of Logistic Regression model\n",
        "lr_metrics_train =  compute_metrics(X_train, y_train, lr_reg, 'LogisticRegression')\n",
        "lr_metrics_train"
      ],
      "metadata": {
        "id": "Zva0a4vBICmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the Evaluation Metrics for test set for Logistic Regression model\n",
        "lr_metrics =  compute_metrics(X_test, y_test, lr_reg, 'LogisticRegression')\n",
        "lr_metrics"
      ],
      "metadata": {
        "id": "1iw3TUEXIFbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Naive Bayes model from sklearn library\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Initialize the model \n",
        "mnb = MultinomialNB(alpha=0.0)\n",
        "\n",
        "# Fit the model\n",
        "mnb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "WVpZjjhdIItO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compute the Evaluation Metrics for train set of Naive Bayes model\n",
        "mnb_metrics_train = compute_metrics(X_train, y_train, mnb, 'Naive Bayes')\n",
        "mnb_metrics_train"
      ],
      "metadata": {
        "id": "QZnxAkMRIMCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Compute the Evaluation Metrics for test set of Naive Bayes model\n",
        "mnb_metrics = compute_metrics(X_test, y_test, mnb, 'Naive Bayes')\n",
        "mnb_metrics"
      ],
      "metadata": {
        "id": "fWjObZHCIRCe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}